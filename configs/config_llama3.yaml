# Llama 3 style dense SOTA model (~15.5M parameters)
vocab_size: 16000
d_model: 256
n_layers: 6
n_heads: 8
n_kv_heads: 2  # GQA is active
d_ff: 1024
max_seq_len: 256

# Advanced Features (Modern Dense)
use_rotary_embeddings: true
use_mixture_of_experts: false
num_experts: 1
num_experts_per_token: 1
use_rezero: true
